@article{Minku2010,
abstract = {Online learning algorithms often have to operate in the presence of concept drift (i.e., the concepts to be learned can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and nonheterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drifts. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide the faster recovery from drifts in long-term.},
author = {Minku, Leandro L. and White, Allan P. and Yao, Xin},
doi = {10.1109/TKDE.2009.156},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Minku, White, Yao - 2010 - The impact of diversity on online ensemble learning in the presence of concept drift.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Concept drift,Diversity,Neural network ensembles,Online learning,artigo,grifado},
mendeley-tags = {artigo,grifado},
number = {5},
pages = {730--742},
title = {{The impact of diversity on online ensemble learning in the presence of concept drift}},
volume = {22},
year = {2010}
}
@article{Pinage2015,
author = {Pinage, Felipe Azevedo and dos Santos, Eulanda Miranda},
doi = {10.1109/ICTAI.2015.152},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinage, Santos - 2015 - A Dissimilarity-Based Drift Detection Method.pdf:pdf},
journal = {2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)},
keywords = {artigo,concept drift,data dissimilarity,dos santos,drift detection,eulanda m,federal university of amazonas,institute of computing,nao{\_}grifado},
mendeley-tags = {artigo,nao{\_}grifado},
pages = {1069--1076},
title = {{A Dissimilarity-Based Drift Detection Method}},
year = {2015}
}
@article{Gama2014,
abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this paper we characterize adaptive learning process, categorize existing strategies for handling concept drift, overview the most representative, distinct and popular techniques and algorithms, discuss evaluation methodology of adaptive algorithms, and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state-of-the-art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts and practitioners.},
archivePrefix = {arXiv},
arxivId = {1010.4784},
author = {Gama, Jo{\~{a}}o and {\v{Z}}liobaitė, Indrė and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
doi = {10.1145/2523813},
eprint = {1010.4784},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gama et al. - 2014 - A survey on concept drift adaptation.pdf:pdf},
isbn = {1011158908},
issn = {03600300},
journal = {{\ldots} Computing Surveys ( {\ldots}},
keywords = {artigo,estudar,nao{\_}grifado},
mendeley-tags = {artigo,estudar,nao{\_}grifado},
number = {4},
pages = {1--37},
pmid = {21824845},
title = {{A survey on concept drift adaptation}},
url = {http://dl.acm.org/citation.cfm?id=2523813{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2597757.2523813},
volume = {46},
year = {2014}
}
@article{Baena-Garcia2006,
abstract = {An emerging problem in Data Streams is the detection of concept drift. This problem is aggravated when the drift is gradual over time. In this work we define a method for detecting concept drift, even in the case of slow gradual change. It is based on the estimated distribution of the distances between classification errors. The proposed method can be used with any learning algorithm in two ways: using it as a wrapper of a batch learning algorithm or implementing it inside an incremental and online algorithm. The experimentation results compare our method (EDDM) with a similar one (DDM). Latter uses the error-rate instead of distance-error-rate},
annote = {NULL},
author = {Baena-Garcia, Manuel and Campo-Avila, Jose Del and Fidalgo, Raul and Bifet, Albert and Gavalda, Ricard and Morales-Bueno, Rafael},
doi = {10.1.1.61.6101},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baena-Garcia et al. - 2006 - Early Drift Detection Method.pdf:pdf},
isbn = {3-540-23237-0},
issn = {0302-9743},
journal = {4th ECML PKDD International Workshop on Knowledge Discovery from Data Streams},
keywords = {artigo,estudar,nao{\_}grifado},
mendeley-tags = {artigo,estudar,nao{\_}grifado},
pages = {77--86},
title = {{Early Drift Detection Method}},
year = {2006}
}
@article{Gama2004,
abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the class-probability distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. A central idea is the concept of context: a set of contiguous examples where the distribution is stationary. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classi ed using the actual model. Statistical theory guarantees that while the distribution is stationary, the error wil decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we de ne a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example kw, and the drift level at example kd. This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since kw. The method was tested with a set of eight arti cial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and also with learning the new concept. We also observe that the method is independent of the learning algorithm.},
annote = {NULL},
author = {Gama, J and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
doi = {10.1007/978-3-540-28645-5_29},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gama et al. - 2004 - Learning with drift detection.pdf:pdf},
journal = {Advances in Artificial Intelligence–SBIA 2004},
keywords = {artigo,concept drift,incremental supervised learning,machine learning,nao{\_}grifado},
mendeley-tags = {artigo,nao{\_}grifado},
pages = {286--295},
title = {{Learning with drift detection}},
year = {2004}
}
@article{Brzezinski2014,
abstract = {Most stream classifiers are designed to process data incrementally, run in resource-aware environments, and react to concept drifts, i.e., unforeseen changes of the stream's underlying data distribution. Ensemble classifiers have become an established research line in this field, mainly due to their modularity which offers a natural way of adapting to changes. However, in environments where class labels are available after each example, ensembles which process instances in blocks do not react to sudden changes sufficiently quickly. On the other hand, ensembles which process streams incrementally, do not take advantage of periodical adaptation mechanisms known from block-based ensembles, which offer accurate reactions to gradual and incremental changes. In this paper, we analyze if and how the characteristics of block and incremental processing can be combined to produce new types of ensemble classifiers. We consider and experimentally evaluate three general strategies for transforming a block ensemble into an incremental learner: online component evaluation, the introduction of an incremental learner, and the use of a drift detector. Based on the results of this analysis, we put forward a new incremental ensemble classifier, called Online Accuracy Updated Ensemble, which weights component classifiers based on their error in constant time and memory. The proposed algorithm was experimentally compared with four state-of-the-art online ensembles and provided best average classification accuracy on real and synthetic datasets simulating different drift scenarios. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Brzezinski, Dariusz and Stefanowski, Jerzy},
doi = {10.1016/j.ins.2013.12.011},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brzezinski, Stefanowski - 2014 - Combining block-based and online methods in learning ensembles from concept drifting data streams.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Concept drift,Data stream,Ensemble,Online classifier,artigo,grifado},
mendeley-tags = {artigo,grifado},
pages = {50--67},
publisher = {Elsevier Inc.},
title = {{Combining block-based and online methods in learning ensembles from concept drifting data streams}},
url = {http://dx.doi.org/10.1016/j.ins.2013.12.011},
volume = {265},
year = {2014}
}
@article{Bifet2007,
abstract = {We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time- and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Naive Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of conditional probabilities in the data. We test our approach using synthetic and real data streams.},
author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
doi = {10.1137/1.9781611972771.42},
file = {:home/regis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bifet, Gavald{\`{a}} - 2007 - Learning from Time-Changing Data with Adaptive Windowing.pdf:pdf},
journal = {Proceedings of the 2007 SIAM International Conference on Data Mining},
pages = {443--448},
title = {{Learning from Time-Changing Data with Adaptive Windowing}},
year = {2007}
}
